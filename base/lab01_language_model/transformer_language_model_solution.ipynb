{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU0wD0LEBbVZ"
   },
   "source": [
    "# Lab 01 : Language Model with Transformers -- solution\n",
    "\n",
    "### Task   \n",
    "\n",
    "The goal is to learn to predict the next word from an input sequence with a language model transformer.\n",
    "\n",
    "The dataset is PTB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13890,
     "status": "ok",
     "timestamp": 1634567669793,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "A3yiLJBBBbVh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe01LtUIBbVj"
   },
   "source": [
    "### GPU\n",
    "\n",
    "It is recommended to run this code on GPU:<br> \n",
    "* Time for 1 epoch on GPU : 48 sec w/ Google Colab Tesla P100-PCIE-16GB <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634567669794,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "g_A4HBT3BbVj",
    "outputId": "7144dbbd-e456-4701-e69d-ddb0ea90f0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# check if GPU or mps is available\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S3W2p84BbVk"
   },
   "source": [
    "### Download Penn Tree Bank\n",
    "\n",
    "The tensor train_data consists of 20 columns of 46,479 words.<br>\n",
    "The tensor test_data consists of 20 columns of 4,121 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2197,
     "status": "ok",
     "timestamp": 1634567671985,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "TnE-JNp_BbVk",
    "outputId": "bc93f6d2-e25f-4761-94bd-17ce6d6c6de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_ptb_dataset_exists\n",
    "data_path=check_ptb_dataset_exists()\n",
    "\n",
    "train_data  =  torch.load(data_path+'ptb/train_data.pt')\n",
    "test_data   =  torch.load(data_path+'ptb/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/atoukoffikougbanhoun/Desktop/WorkSpace/LLM/base_model/base/lab01_language_model\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDxTJimKBbVl"
   },
   "source": [
    "### Some constants associated with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634567671986,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "MS9EUUvbBbVm"
   },
   "outputs": [],
   "source": [
    "bs = 20\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U9UvE28BbVm"
   },
   "source": [
    "### Make an attention net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634567671988,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "RRmg_OUaBbVn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_positional_encoding(seq_length, dim):\n",
    "    assert dim == 2* (dim//2) # check if dim is divisible by 2\n",
    "    pe = torch.zeros(seq_length, dim)\n",
    "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe        \n",
    "   \n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d, d_head, dropout):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d_head)\n",
    "        self.LN_MLP = nn.LayerNorm(d_head)\n",
    "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
    "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
    "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        H_in = H # Add residual connection (RC)\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d]        \n",
    "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
    "        attention_score = Q @ K.transpose(2,1) * Q.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        mask = torch.tril(torch.ones(batch_len,batch_len)).long().to(device) # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
    "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
    "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
    "        H_HA = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        return H_HA # return prediction scores for next token\n",
    "\n",
    "class MultipleAttentionHead(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n",
    "        assert d == d_head * num_heads # check divisibility\n",
    "        self.MHA = nn.ModuleList([ AttentionHead(d, d_head, dropout) for _ in range(num_heads) ])\n",
    "        self.WO = nn.Linear(d, d) # combination layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); seq_length = H.size(1)\n",
    "        H_heads = []\n",
    "        for HA_layer in self.MHA:\n",
    "            H_heads.append(HA_layer(H)) # size=[batch_size, seq_length, d_head]\n",
    "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]            \n",
    "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
    "        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n",
    "        return H\n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d)\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "        self.MHA = MultipleAttentionHead(d, num_heads, dropout)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))        \n",
    "    def forward(self, H): # size=[batch_size, seq_length, d]\n",
    "        # Multiple Attention Heads w/ layer normalization (LN), residual connection (RC)\n",
    "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, seq_length, d]\n",
    "        # MLP w/ layer normalization (LN), residual connection (RC)\n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, seq_length, d]\n",
    "        return H # size=[batch_size, seq_length, d]\n",
    "        \n",
    "        \n",
    "class Transformer_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.TR_Blocks = nn.ModuleList([ TransformerBlock(d, num_heads, dropout) for _ in range(num_blocks) ]) \n",
    "    def forward(self, batch_seq, pos_enc):\n",
    "        H = batch_seq.transpose(1,0) # size=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Add positional encoding  \n",
    "        pos_enc = pos_enc.unsqueeze(dim=0) # size=[1,          seq_length, d]\n",
    "        H = H + pos_enc                    # size=[batch_size, seq_length, d]\n",
    "        # Apply transformer blocks \n",
    "        for TR_Block in self.TR_Blocks:\n",
    "            H = TR_Block(H)\n",
    "        # Output\n",
    "        H = H.permute(1,0,2)  # size=[batch_length, batch_size, d]\n",
    "        return H # return prediction scores for next token\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super(ANN, self).__init__()\n",
    "        self.decoder = Transformer_decoder(d, num_heads, num_blocks, seq_length, dropout)\n",
    "    \n",
    "    def forward(self, g_seq , pos ):\n",
    "        h_dec_seq = self.decoder( g_seq , pos )\n",
    "        return h_dec_seq \n",
    "    \n",
    "\n",
    "class attention_net(nn.Module):\n",
    "\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super(attention_net, self).__init__()  \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = ANN(d, num_heads, num_blocks, seq_length, dropout)\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "    def forward(self, word_seq, pos ):\n",
    "        g_seq     =   self.layer1( word_seq ) # size=(seq_length, bs, hidden_dim) \n",
    "        h_seq     =   self.layer2( g_seq , pos ) # size=(seq_length, bs, hidden_dim) \n",
    "        score_seq =   self.layer3( h_seq ) # size=(seq_length, bs, vocab_size)\n",
    "        return score_seq \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4FH3SgyBbVt"
   },
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634567671989,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "wcOReFN0IHcP"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        pos = generate_positional_encoding(seq_length, hidden_size)\n",
    "        \n",
    "        minibatch_data = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "        pos = pos.to(device)\n",
    "\n",
    "        scores = net( minibatch_data, pos )\n",
    "        \n",
    "        minibatch_label = minibatch_label.view(  bs*seq_length ) \n",
    "        scores = scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gnTKxy9BbVr"
   },
   "source": [
    "### Build the net. Choose the hidden size to be 128, the number of heads to be 4, and the number of blocks 2. \n",
    "### How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634567671989,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "ADOYeTS6_SLO",
    "outputId": "1283bbd3-e46b-462e-f692-9273e5b0c45a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_net(\n",
      "  (layer1): Embedding(10000, 128)\n",
      "  (layer2): ANN(\n",
      "    (decoder): Transformer_decoder(\n",
      "      (TR_Blocks): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (LN_MHA): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (LN_MLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (MHA): MultipleAttentionHead(\n",
      "            (MHA): ModuleList(\n",
      "              (0-3): 4 x AttentionHead(\n",
      "                (LN_MHA): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (LN_MLP): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (query): Linear(in_features=128, out_features=32, bias=False)\n",
      "                (key): Linear(in_features=128, out_features=32, bias=False)\n",
      "                (value): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (dropout): Dropout(p=0.95, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.95, inplace=False)\n",
      "          )\n",
      "          (MLP): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.95, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Linear(in_features=128, out_features=10000, bias=True)\n",
      ")\n",
      "There are 2967056 (2.97 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128 \n",
    "num_heads = 4\n",
    "num_blocks = 2\n",
    "dropout = 0.95\n",
    "seq_length = 100\n",
    "\n",
    "net = attention_net(hidden_size, num_heads, num_blocks, seq_length, dropout)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khaDFxmwBbVr"
   },
   "source": [
    "### Send the network to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9482,
     "status": "ok",
     "timestamp": 1634567681462,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "Sp63FT_v_SLQ"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecX1xVaBbVs"
   },
   "source": [
    "### Choose the loss to be the cross-entropy and the optimizer to be Adam, as well as the following important hyperparameters: \n",
    "* initial learning rate = 0.001\n",
    "* sequence length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1634567681465,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "LBm4Aw_j_SLR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
    "\n",
    "pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frtpNDBLBbVs"
   },
   "source": [
    "### Do 5 passes through the training set\n",
    "### Observe the train perplexity and the test perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512024,
     "status": "ok",
     "timestamp": 1634568193467,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "McQc1_xkBbVp",
    "outputId": "965c9de2-04f8-4530-e820-2a207e008eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 12.252074956893921 \t lr= 0.00021762913579014855 \t exp(loss)= 47.75411980533963\n",
      "test: exp(loss) =  198.86872910301688\n",
      "\n",
      "epoch= 1 \t time= 24.949244022369385 \t lr= 0.00021762913579014855 \t exp(loss)= 46.808071795932634\n",
      "test: exp(loss) =  201.7638837078004\n",
      "\n",
      "epoch= 2 \t time= 37.64756107330322 \t lr= 0.00019784466890013504 \t exp(loss)= 45.74088648725345\n",
      "test: exp(loss) =  204.05088999030417\n",
      "\n",
      "epoch= 3 \t time= 50.35393691062927 \t lr= 0.00017985878990921367 \t exp(loss)= 44.77842558755509\n",
      "test: exp(loss) =  205.866611376765\n",
      "\n",
      "epoch= 4 \t time= 63.071797132492065 \t lr= 0.00016350799082655786 \t exp(loss)= 43.917690402149\n",
      "test: exp(loss) =  207.79286331998063\n",
      "\n",
      "epoch= 5 \t time= 75.7859559059143 \t lr= 0.0001486436280241435 \t exp(loss)= 43.14814033926362\n",
      "test: exp(loss) =  209.65413923664576\n",
      "\n",
      "epoch= 6 \t time= 88.49476408958435 \t lr= 0.00013513057093103952 \t exp(loss)= 42.46079378528039\n",
      "test: exp(loss) =  211.39795050339245\n",
      "\n",
      "epoch= 7 \t time= 101.21930289268494 \t lr= 0.0001228459735736723 \t exp(loss)= 41.84151740626995\n",
      "test: exp(loss) =  212.86954433490615\n",
      "\n",
      "epoch= 8 \t time= 113.94019198417664 \t lr= 0.00011167815779424753 \t exp(loss)= 41.28738779518084\n",
      "test: exp(loss) =  214.3077352928729\n",
      "\n",
      "epoch= 9 \t time= 126.76136875152588 \t lr= 0.00010152559799477047 \t exp(loss)= 40.78660575958953\n",
      "test: exp(loss) =  215.64329682113504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "start=time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch >= 2:\n",
    "        optimizer.param_groups[0]['lr'] /= 1.1 \n",
    "        my_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    for count in range( 0 , 46478-seq_length ,  seq_length):\n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch and the positional encoding\n",
    "        minibatch_data = train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]    \n",
    "        pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim) \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "        pos = pos.to(device)\n",
    "        \n",
    "        # forward the minibatch through the net        \n",
    "        scores = net( minibatch_data, pos ) # size=(seq_length, bs, vocab_size)\n",
    "\n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores = scores.view(  bs*seq_length , vocab_size) # size=(seq_length/2.bs, vocab_size)\n",
    "        minibatch_label = minibatch_label.view(  bs*seq_length ) # size=(seq_length/2.bs, vocab_size)\n",
    "       \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "JRu41IdwBbVt",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Choose one sentence (taken from the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1634568349512,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "gSmC_QyMBbVt"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"some analysts expect oil prices to remain relatively\"\n",
    "\n",
    "sentence2 = \"over the next days and weeks they say investors should look for stocks to\"\n",
    "\n",
    "sentence3 = \"prices averaging roughly $ N a barrel higher in the third\"\n",
    "\n",
    "sentence4 = \"i think my line has been very consistent mrs. hills said at a news\"\n",
    "\n",
    "sentence5 = \"this appears particularly true at gm which had strong sales in\"\n",
    "\n",
    "# or make your own sentence.  No capital letter or punctuation allowed. Each word must be in the allowed vocabulary.\n",
    "sentence6 = \"be your self,the world\"\n",
    "\n",
    "# SELECT THE SENTENCE HERE\n",
    "mysentence = sentence6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoC9H2zBBbVu"
   },
   "source": [
    "### Display the the network prediction for the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1634568352272,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "YenIJTXcBbVu",
    "outputId": "7762d1de-e6e2-4ea6-c914-78ab474e106d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered a word which is not in the vocabulary.\n",
      "Make sure that you do not have any capital letters\n",
      "be your self,the world ... \n",
      "\n",
      "37.0%\t <eos>\n",
      "20.9%\t war\n",
      "8.2%\t 's\n",
      "6.1%\t series\n",
      "5.4%\t with\n",
      "1.9%\t <unk>\n",
      "1.6%\t and\n",
      "1.4%\t of\n",
      "1.2%\t bank\n",
      "0.8%\t at\n",
      "0.8%\t for\n",
      "0.7%\t the\n",
      "0.7%\t capital\n",
      "0.7%\t as\n",
      "0.6%\t by\n",
      "0.6%\t said\n",
      "0.5%\t in\n",
      "0.4%\t new\n",
      "0.4%\t according\n",
      "0.4%\t a\n",
      "0.4%\t private\n",
      "0.3%\t central\n",
      "0.3%\t to\n",
      "0.3%\t that\n",
      "0.3%\t but\n",
      "0.2%\t based\n",
      "0.2%\t quite\n",
      "0.2%\t mortgage\n",
      "0.2%\t after\n",
      "0.2%\t announced\n"
     ]
    }
   ],
   "source": [
    "minibatch_data = utils.sentence2vector(mysentence)\n",
    "minibatch_data = torch.cat((minibatch_data, minibatch_data), dim=0) # copy-paste the test sequence to use the same attention window size for each word\n",
    "pos = generate_positional_encoding(minibatch_data.size(0), hidden_size) \n",
    "\n",
    "minibatch_data = minibatch_data.to(device)\n",
    "pos = pos.to(device)   \n",
    "\n",
    "net.eval()\n",
    "scores = net( minibatch_data, pos )\n",
    "scores = scores[-1,:] # select the last score vector for the prediction of the next word from the input sequence\n",
    "scores = scores[0].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(mysentence, '... \\n')\n",
    "utils.show_next_word(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634568352273,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "-g3t66JccXCq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_language_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
