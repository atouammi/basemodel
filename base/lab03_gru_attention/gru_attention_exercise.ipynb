{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F_kTNpUg_tP"
   },
   "source": [
    "# Lab 03 : Gated Recurrent Units (GRUs) with Attention -- exercise\n",
    "\n",
    "#### Task\n",
    "\n",
    "Implement GRU-based seq2seq model with Luong attention (https://arxiv.org/pdf/1508.04025.pdf) and train the model on the french to english translation dataset. (the use of the function nn.GRU() is allowed but the attention scheme needs to be implemented **explicitly**) :\n",
    "\n",
    "The Luong attention algorithm performs the following operations:\n",
    "\n",
    "1. The encoder generates a set of hidden states, $H = \\textbf{h}_i, i = 1, 2, .....T$ , from the input sentence. The decoder generates a set of hidden states, $S = \\textbf{s}_t, t =1, 2, .....$.\n",
    "2. The current decoder hidden state is computed as: $\\textbf{s}_t = GRU_{decoder}(\\textbf{s}_{t-1}, y_{t-1})$. Here, $\\textbf{s}_{t-1}$ denotes the previous hidden decoder state, and $y_{t-1}$ the current input, which is also the expected output for the previous timestep.\n",
    "\n",
    "3. A dot product on the encoder hidden state $\\textbf{h}_i$ and the current decoder hidden state $\\textbf{s}_t$ to compute the alignment scores: $e_{t,i} = \\textbf{s}_t . \\textbf{h}_i$.\n",
    "\n",
    "4. A softmax function is applied to the alignment scores, effectively normalizing them into attention weights in a range between 0 and 1: $\\alpha_{t, i} = \\text{softmax}(e_{t, i}/ \\textbf{e}_t)$.\n",
    "\n",
    "5. These attention weights together with the encoder hidden states are used to generate a context vector through a weighted sum: $\\textbf{c}_t = \\sum_{i=1}^T\\alpha_{t, i}\\textbf{h}_i$.\n",
    "\n",
    "6. An attentional hidden state is computed based on a weighted concatenation of the context vector and the current decoder hidden state: $\\tilde{\\textbf{s}_t} = \\text{tanh}\\big(W_c\\big[\\textbf{c}_t; \\textbf{s}_t\\big]\\big)$.\n",
    "\n",
    "7. The decoder produces a final output by feeding it a weighted attentional hidden state: $y_t = \\text{softmax}(W_y\\tilde{\\textbf{s}_t})$.\n",
    "\n",
    "8. Steps 2-7 are repeated until the end of the sequence.\n",
    "\n",
    "The attention has to be calculated in parallel via matrix multiplication. For loop $\\textbf{should not}$ be used.\n",
    "\n",
    "**Hints:**\n",
    "1. torch.swapaxes or torch.transpose to convert from [seq_len, bs, hidden_size] to [bs, seq_len, hidden_size].\n",
    "1. torch.bmm to perform batch matrix multiplication\n",
    "1. torch.concat to concatenate $c_t$ and $s_t$\n",
    "1. Training took around ~1 minute per epoch\n",
    "\n",
    "*Prepared by Liu Xiaokang with the contribution of Chew Kin Whye*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242_2025_codes/labs_lecture07/lab03_gru_attention'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Download\n",
    "\n",
    "The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It includes versions in 21 European languages.\n",
    "\n",
    "In this lab, we focus on translation between English and French. Please download the dataset by executing the following cell. (The downloading process may take a while)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import wget\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
    "    import wget\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "url = 'https://www.statmt.org/europarl/v7/fr-en.tgz'\n",
    "file_path = './'\n",
    "\n",
    "if not os.path.exists(os.path.join(file_path, \"fr-en.tgz\")):\n",
    "    downloaded_file = wget.download(url, file_path)\n",
    "\n",
    "    # Extract the .tgz file\n",
    "    with tarfile.open(downloaded_file, 'r:gz') as tar:\n",
    "        tar.extractall(path=file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Spacy if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except ImportError:\n",
    "    !pip install spacy==3.7.5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class\n",
    "\n",
    "This cell may take a while to executes as it tries to download the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mz8m4f5iwWO",
    "outputId": "62e2422b-ffe8-4425-c2ae-bd33eae3193e"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Make sure the spaCy models are downloaded\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"fr_core_news_sm\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "# For this dataset, we are trying to translate french to english\n",
    "SRC_LANGUAGE = 'fr'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Custom dataset for french-to-english translation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, train_size=10000, test_size=1000, max_len=50):\n",
    "        self.en_dir = \"europarl-v7.fr-en.en\"\n",
    "        self.fr_dir = \"europarl-v7.fr-en.fr\"\n",
    "        with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.english_data = f.readlines()\n",
    "        with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
    "            self.french_data = f.readlines()\n",
    "        # Only select sentences with length less than max_len\n",
    "        self.indicies = np.array([i for i in range(len(self.english_data)) if len(self.english_data[i]) < max_len])\n",
    "        if train:\n",
    "            self.english_data = [self.english_data[i] for i in self.indicies][:train_size]\n",
    "            self.french_data = [self.french_data[i] for i in self.indicies][:train_size]\n",
    "        else:\n",
    "            self.english_data = [self.english_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "            self.french_data = [self.french_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.french_data[idx], self.english_data[idx]\n",
    "\n",
    "# Instantiate dataset\n",
    "# dataset = CustomDataset(train=True)\n",
    "train_size = 2500\n",
    "test_size = 250\n",
    "dataset = CustomDataset(train=True, train_size=train_size, test_size=test_size)\n",
    "\n",
    "\n",
    "# A simple get_tokenizer function using spaCy directly\n",
    "def get_tokenizer(model: str, language: str):\n",
    "    # Load the appropriate spaCy model\n",
    "    nlp = spacy.load(model)\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        return [token.text for token in nlp(text)]\n",
    "    return tokenize\n",
    "\n",
    "# Load tokenizers for source and target languages\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('fr_core_news_sm', SRC_LANGUAGE)\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('en_core_web_sm', TGT_LANGUAGE)\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, token_to_index: dict, default_index: int = 0):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.default_index = default_index\n",
    "        self.index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "\n",
    "    def __call__(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.token_to_index.get(token, self.default_index) for token in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def set_default_index(self, index: int):\n",
    "        self.default_index = index\n",
    "\n",
    "    def lookup_token(self, index: int) -> str:\n",
    "        return self.index_to_token.get(index, self.index_to_token.get(self.default_index, '<unk>'))\n",
    "\n",
    "\n",
    "# Function to build a vocabulary from an iterator over token lists\n",
    "def build_vocab_from_iterator(iterator: Iterable[List[str]], min_freq: int = 1,\n",
    "                              specials: List[str] = None, special_first: bool = True) -> Vocab:\n",
    "    if specials is None:\n",
    "        specials = []\n",
    "    counter = Counter()\n",
    "    for token_list in iterator:\n",
    "        counter.update(token_list)\n",
    "    # Filter tokens below frequency threshold\n",
    "    tokens = [token for token, freq in counter.items() if freq >= min_freq]\n",
    "    tokens = sorted(tokens)\n",
    "    if special_first:\n",
    "        final_tokens = specials + tokens\n",
    "    else:\n",
    "        final_tokens = tokens + specials\n",
    "    # Create token to index mapping\n",
    "    token_to_index = {token: idx for idx, token in enumerate(final_tokens)}\n",
    "    # Use the index of the first special token as default (usually <unk>)\n",
    "    default_index = token_to_index[specials[0]] if specials else 0\n",
    "    return Vocab(token_to_index, default_index)\n",
    "\n",
    "# Helper function to yield tokens from the dataset iterator\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> Iterable[List[str]]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Build vocabulary for both languages\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = iter(dataset)\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                      min_freq=1,\n",
    "                                                      specials=special_symbols,\n",
    "                                                      special_first=True)\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "\n",
    "# Functions to transform input sentences for training\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    # Convert list of token indices to a tensor and add BOS/EOS tokens\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# Build text_transform dictionary combining tokenization, numericalization and tensor conversion\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln],  # Tokenization\n",
    "                                               vocab_transform[ln],  # Numericalization\n",
    "                                               tensor_transform)     # Add BOS/EOS tokens\n",
    "\n",
    "def collate_fn(src, tgt):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in zip(src, tgt):\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99DcG-uAGa3b",
    "outputId": "38af8148-9eab-4af5-dbff-b01ddb693130"
   },
   "outputs": [],
   "source": [
    "# Print an example\n",
    "batch_size = 8\n",
    "dataset = CustomDataset(train=True, train_size=train_size, test_size=test_size)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "fr_sentence, eng_sentence = next(iter(train_dataloader))\n",
    "print(f\"Raw Inputs: {fr_sentence[0]}\\n{eng_sentence[0]}\")\n",
    "# First we split the sentence into tokens\n",
    "fr_token, eng_token = [token_transform[\"fr\"](i.rstrip(\"\\n\")) for i in fr_sentence], [token_transform[\"en\"](i.rstrip(\"\\n\")) for i in eng_sentence]\n",
    "print(f\"Tokenized Inputs: {fr_token[0]}\\n{eng_token[0]}\")\n",
    "# # Next we transform the tokens into numbers\n",
    "fr_idx, eng_idx = [vocab_transform[\"fr\"](i) for i in fr_token], [vocab_transform[\"en\"](i) for i in eng_token]\n",
    "print(f\"Tokenized Inputs to indicies: {fr_idx[0]}\\n{eng_idx[0]}\")\n",
    "# # Next, we add the beginning of sentence, end of sentence\n",
    "fr_pad, eng_pad = [tensor_transform(i) for i in fr_idx], [tensor_transform(i) for i in eng_idx]\n",
    "print(f\"Tokenized Indicies with begin (2) and end token (3): {fr_pad[0]}\\n{eng_pad[0]}\")\n",
    "# # Lastly, we pad the rest of the sentence\n",
    "# This also changes the shape from (bs, seq_len) to (seq_len, bs)\n",
    "fr_pad, eng_pad = pad_sequence(fr_pad, padding_value=PAD_IDX), pad_sequence(eng_pad, padding_value=PAD_IDX)\n",
    "print(f\"After padding (1): {fr_pad[:, 0]}\\n{eng_pad[:, 0]}\")\n",
    "\n",
    "# All the above is combined into collate_fn\n",
    "x, y = collate_fn(fr_sentence, eng_sentence)\n",
    "print(f\"Same Outputs: {x[:, 0]}\\n{y[:, 0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61CLe7y-x9P7",
    "outputId": "9f2f71f3-0732-49d3-bb62-d9f2d0e3edd8"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import utils\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "hidden_size = 256\n",
    "my_lr = 1.3\n",
    "bs = 32\n",
    "\n",
    "# Variables\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "train_dataset = CustomDataset(train=True, train_size=train_size, test_size=test_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = CustomDataset(train=False, train_size=train_size, test_size=test_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    h=h.to(device)\n",
    "\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = collate_fn(x, y)\n",
    "        # Batch size might be different for the last batch\n",
    "        batch_size = x.size()[1]\n",
    "        seq_length = y.size()[0] - 1\n",
    "        # set the initial h to be the zero vector\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        # send them to the gpu\n",
    "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "        h=h.to(device)\n",
    "\n",
    "        # COMPLETE HERE \n",
    "        scores = \n",
    "        minibatch_label = \n",
    "        loss = \n",
    "        # COMPLETE HERE \n",
    "\n",
    "        loss = criterion(scores ,  minibatch_label )\n",
    "\n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # update the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        # Collect garbage to prevent OOM\n",
    "        gc.collect()\n",
    "\n",
    "    total_loss = running_loss/num_batches\n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    return math.exp(total_loss)\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.source_embedding = nn.Embedding( SRC_VOCAB_SIZE, hidden_size )\n",
    "        self.target_embedding = nn.Embedding( TGT_VOCAB_SIZE, hidden_size )\n",
    "        # COMPLETE HERE \n",
    "        self.gru_encoder =\n",
    "        self.gru_decoder =\n",
    "        self.W_c =\n",
    "        self.W_y =\n",
    "        # COMPLETE HERE \n",
    "\n",
    "    def forward(self, x, y, h_init):\n",
    "        # x.shape = (15, 32)\n",
    "        source_seq = self.source_embedding(x)  #(15, 32, 256)\n",
    "        target_seq = self.target_embedding(y)  # (13, 32, 256)\n",
    "        # COMPLETE HERE \n",
    "\n",
    "\n",
    "        # COMPLETE HERE \n",
    "\n",
    "        return \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available with GPU:',torch.cuda.get_device_name(0))\n",
    "\n",
    "net = LuongAttention( hidden_size )\n",
    "utils.display_num_param(net)\n",
    "\n",
    "print(net)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "print('')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:\n",
    "        for p in net.parameters():\n",
    "            p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "      # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "\n",
    "    # create a new optimizer and give the current learning rate.\n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "\n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        # Transform inputs\n",
    "        x, y = collate_fn(x, y)\n",
    "\n",
    "        # Batch size might be different for the last batch\n",
    "        batch_size = x.size()[1]\n",
    "        seq_length = y.size()[0] - 1\n",
    "        # set the initial h to be the zero vector\n",
    "        h = torch.zeros(1, batch_size, hidden_size)\n",
    "        # send them to the gpu\n",
    "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "\n",
    "        h=h.to(device)\n",
    "\n",
    "        try:\n",
    "            # COMPLETE HERE \n",
    "            scores = \n",
    "            scores = \n",
    "            minibatch_label = \n",
    "            # COMPLETE HERE \n",
    "\n",
    "            loss = criterion(scores ,  minibatch_label )\n",
    "        except RuntimeError:\n",
    "            pdb.set_trace()\n",
    "        h=h.detach()\n",
    "\n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        # update the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "        # Collect garbage to prevent OOM\n",
    "        gc.collect()\n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWjgSSE1wBOf"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "count = 0\n",
    "for x, y in test_dataloader:\n",
    "    print(x)\n",
    "    h = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "    h=h.to(device)\n",
    "    x, y = collate_fn(x, y)\n",
    "    minibatch_data=x.type(torch.LongTensor).to(device)\n",
    "    start_index = torch.tensor([[2]]).type(torch.LongTensor).to(device)\n",
    "    predictions=start_index\n",
    "    for _ in range(20):\n",
    "        predictions = net.forward(minibatch_data, predictions, h)\n",
    "        predictions = torch.reshape(predictions, (-1, TGT_VOCAB_SIZE, 1))\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        predictions = torch.cat([start_index, predictions], 0)\n",
    "        if predictions[-1].item() == 3:\n",
    "            break\n",
    "    predictions = predictions.reshape(-1)\n",
    "    predictions = [vocab_transform[TGT_LANGUAGE].lookup_token(i.item()) for i in list(predictions)]\n",
    "    print(f\"Label: {[vocab_transform[TGT_LANGUAGE].lookup_token(i.item()) for i in y]}\")\n",
    "    print(f\"Predicted: {predictions}\")\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "pa1frUWBezF3",
    "l3Aot53UezF4"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
