{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU0wD0LEBbVZ"
   },
   "source": [
    "# Lab 02 : Sequence-to-sequence transformers -- solution\n",
    "\n",
    "### Task   \n",
    "\n",
    "The goal is to learn to translate an input sequence to an output sequence, which is simply the same input sequence but shifted to the right by one word.  \n",
    "\n",
    "Example, if the input sequence is \"some analysts expect oil prices to remain relatively\" then the output sequence is \"analysts expect oil prices to remain relatively high\".  \n",
    "\n",
    "We will use an encoder-decoder Transformer to achieve this goal on the PTB dataset. The decoder will start with a token \"start_token\" assigned to the 10,001-th word in the dictionary. \n",
    "\n",
    "Example, if the input sequence is \"456 82 948 5892 34 4928 4758 567\" then the output sequence is \"10001 82 948 5892 34 4928 4758 567\" and  the label sequence is \"82 948 5892 34 4928 4758 567 745\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1634566698702,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "qoogmpRzBbVg",
    "outputId": "5d1cf4ca-264b-4d9d-da9b-5cb4f49ad097"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242_2025_codes/labs_lecture07/lab02_seq2seq/'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1634566699273,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "A3yiLJBBBbVh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1634566699274,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "g_A4HBT3BbVj",
    "outputId": "780cb794-c757-4952-828b-31fd84392780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda available with GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available with GPU:',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S3W2p84BbVk"
   },
   "source": [
    "### Download Penn Tree Bank\n",
    "\n",
    "The tensor train_data consists of 20 columns of 46,479 words.<br>\n",
    "The tensor test_data consists of 20 columns of 4,121 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1634566699274,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "TnE-JNp_BbVk",
    "outputId": "a59a9495-c958-4d68-e082-ffacb35f6a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_ptb_dataset_exists\n",
    "data_path=check_ptb_dataset_exists()\n",
    "\n",
    "train_data  =  torch.load(data_path+'ptb/train_data.pt')\n",
    "test_data   =  torch.load(data_path+'ptb/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1001, 20])\n"
     ]
    }
   ],
   "source": [
    "# Extract a sub-part of PTB\n",
    "doc_len = 1001\n",
    "train_data = train_data[:doc_len,:]\n",
    "print(  train_data.size()  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDxTJimKBbVl"
   },
   "source": [
    "### Some constants associated with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1634566699275,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "MS9EUUvbBbVm"
   },
   "outputs": [],
   "source": [
    "bs = 20\n",
    "vocab_size = 10000 + 1 # +1 for the start token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U9UvE28BbVm"
   },
   "source": [
    "### Make an attention net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634566699276,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "RRmg_OUaBbVn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_positional_encoding(seq_length, dim):\n",
    "    assert dim == 2* (dim//2) # check if dim is divisible by 2\n",
    "    pe = torch.zeros(seq_length, dim)\n",
    "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe        \n",
    "    \n",
    "\n",
    "########### Encoder Transformer Block ###########\n",
    "class AttentionHead_encoder(nn.Module):\n",
    "    def __init__(self, d, d_head, dropout):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
    "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
    "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # self-attention encoder\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d] \n",
    "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
    "        attention_score = Q @ K.transpose(2,1) * Q.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
    "        H_HA = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        return H_HA # return prediction scores for next token\n",
    "        \n",
    "class MultipleAttentionHead_encoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n",
    "        assert d == d_head * num_heads # check divisibility\n",
    "        self.MHA = nn.ModuleList([ AttentionHead_encoder(d, d_head, dropout) for _ in range(num_heads) ])\n",
    "        self.WO = nn.Linear(d, d) # combination layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); seq_length = H.size(1)\n",
    "        H_heads = []\n",
    "        for HA_layer in self.MHA:\n",
    "            H_heads.append(HA_layer(H)) # size=[batch_size, seq_length, d_head]\n",
    "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]            \n",
    "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
    "        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n",
    "        return H\n",
    "\n",
    "class TransformerBlock_encoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d)\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "        self.MHA = MultipleAttentionHead_encoder(d, num_heads, dropout)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))   \n",
    "    def forward(self, H): # size=[batch_size, seq_length, d]\n",
    "        # Self-attention encoder \n",
    "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, seq_length, d], [20, 30, 128]\n",
    "        # MLP \n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, seq_length, d]\n",
    "        return H # size=[batch_size, seq_length, d]\n",
    "    \n",
    "class Transformer_encoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.TR_Blocks = nn.ModuleList([ TransformerBlock_encoder(d, num_heads, dropout) for _ in range(num_blocks) ]) \n",
    "    def forward(self, batch_seq, pos_enc):\n",
    "        H = batch_seq.transpose(1,0) # size=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Add positional encoding  \n",
    "        pos_enc = pos_enc.unsqueeze(dim=0) # size=[1,          seq_length, d]\n",
    "        H = H + pos_enc                    # size=[batch_size, seq_length, d]\n",
    "        # Apply transformer blocks \n",
    "        for TR_Block in self.TR_Blocks:\n",
    "            H = TR_Block(H)\n",
    "        # Output\n",
    "        H = H.permute(1,0,2)  # size=[batch_length, batch_size, d]\n",
    "        return H # return prediction scores for next token\n",
    "########### Encoder Transformer Block ###########\n",
    "    \n",
    "\n",
    "########### Decoder Transformer Block ###########\n",
    "class SelfAttention_AttentionHead_decoder(nn.Module):\n",
    "    def __init__(self, d, d_head, dropout):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
    "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
    "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Masked self-attention decoder\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d] \n",
    "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
    "        attention_score = Q @ K.transpose(2,1) * Q.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        mask = torch.tril(torch.ones(batch_len,batch_len)).long().to(device) # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
    "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
    "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
    "        H_HA = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        return H_HA # return prediction scores for next token\n",
    "    \n",
    "class SelfAttention_MultipleAttentionHead_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n",
    "        assert d == d_head * num_heads # check divisibility\n",
    "        self.MHA = nn.ModuleList([ SelfAttention_AttentionHead_decoder(d, d_head, dropout) for _ in range(num_heads) ])\n",
    "        self.WO = nn.Linear(d, d) # combination layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); seq_length = H.size(1)\n",
    "        H_heads = []\n",
    "        for HA_layer in self.MHA:\n",
    "            H_heads.append(HA_layer(H)) # size=[batch_size, seq_length, d_head]\n",
    "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]            \n",
    "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
    "        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n",
    "        return H\n",
    "\n",
    "class CrossAttention_AttentionHead_decoder(nn.Module):\n",
    "    def __init__(self, d, d_head, dropout):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
    "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
    "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H, Henc): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Masked cross-attention\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d]  \n",
    "        K = self.key(Henc) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(Henc) # size=[batch_size, batch_length, d]\n",
    "        attention_score = Q @ K.transpose(2,1) * Q.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
    "        H_HA = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        return H_HA # return prediction scores for next token\n",
    "        \n",
    "class CrossAttention_MultipleAttentionHead_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n",
    "        assert d == d_head * num_heads # check divisibility\n",
    "        self.MHA = nn.ModuleList([ CrossAttention_AttentionHead_decoder(d, d_head, dropout) for _ in range(num_heads) ])\n",
    "        self.WO = nn.Linear(d, d) # combination layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, H, Henc): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); seq_length = H.size(1)\n",
    "        H_heads = []\n",
    "        for HA_layer in self.MHA:\n",
    "            H_heads.append(HA_layer(H, Henc)) # size=[batch_size, seq_length, d_head]\n",
    "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]            \n",
    "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
    "        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n",
    "        return H\n",
    "    \n",
    "class TransformerBlock_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.LN_MHA_H = nn.LayerNorm(d)\n",
    "        self.LN_MHA_Henc = nn.LayerNorm(d)\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "        self.SA_MHA = SelfAttention_MultipleAttentionHead_decoder(d, num_heads, dropout)\n",
    "        self.CA_MHA = CrossAttention_MultipleAttentionHead_decoder(d, num_heads, dropout)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))   \n",
    "    def forward(self, H, Henc): # size=[batch_size, seq_length, d]\n",
    "        # Masked self-attention decoder\n",
    "        H = H + self.SA_MHA(self.LN_MHA_H(H)) # size=[batch_size, seq_length, d], [20, 30, 128]\n",
    "        # Masked cross-attention decoder\n",
    "        H = H + self.CA_MHA(self.LN_MHA_H(H),self.LN_MHA_Henc(Henc)) # size=[batch_size, seq_length, d], [20, 30, 128]\n",
    "        # MLP \n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, seq_length, d]\n",
    "        return H # size=[batch_size, seq_length, d]\n",
    "           \n",
    "class Transformer_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.TR_Blocks = nn.ModuleList([ TransformerBlock_decoder(d, num_heads, dropout) for _ in range(num_blocks) ]) \n",
    "    def forward(self, g_seq_out, h_enc_seq, pos_enc):\n",
    "        H = g_seq_out.transpose(1,0) # size=[batch_size, seq_length, d]\n",
    "        Henc = h_enc_seq.transpose(1,0) # size=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Add positional encoding  \n",
    "        pos_enc = pos_enc.unsqueeze(dim=0) # size=[1,          seq_length, d]\n",
    "        H = H + pos_enc                    # size=[batch_size, seq_length, d]\n",
    "        # Apply transformer blocks \n",
    "        for TR_Block in self.TR_Blocks:\n",
    "            H = TR_Block(H, Henc)\n",
    "        # Output\n",
    "        H = H.permute(1,0,2)  # size=[batch_length, batch_size, d]\n",
    "        return H # return prediction scores for next token\n",
    "########### Decoder Transformer Block ###########\n",
    "\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super(ANN, self).__init__()\n",
    "        self.encoder = Transformer_encoder(d, num_heads, num_blocks, seq_length, dropout)\n",
    "        self.decoder = Transformer_decoder(d, num_heads, num_blocks, seq_length, dropout)\n",
    "    \n",
    "    def forward(self, g_seq_in , g_seq_out, pos ):\n",
    "        h_enc_seq = self.encoder( g_seq_in , pos ) # size=[batch_size, seq_length, d], [30, 20, 128]\n",
    "        h_dec_seq = self.decoder( g_seq_out, h_enc_seq , pos )  # size=[batch_size, seq_length, d], [30, 20, 128]\n",
    "        return h_dec_seq \n",
    "    \n",
    "\n",
    "class attention_net(nn.Module):\n",
    "\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
    "        super(attention_net, self).__init__()  \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = ANN(d, num_heads, num_blocks, seq_length, dropout)\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "    def forward(self, word_seq_in, word_seq_out, pos ):\n",
    "        g_seq_in     =   self.layer1( word_seq_in ) # size=(seq_length, bs, hidden_dim), [30, 20, 128]\n",
    "        g_seq_out     =   self.layer1( word_seq_out ) # size=(seq_length, bs, hidden_dim), [30, 20, 128]\n",
    "        h_seq     =   self.layer2( g_seq_in , g_seq_out, pos ) # size=(seq_length, bs, hidden_dim), [30, 20, 128] \n",
    "        score_seq =   self.layer3( h_seq ) # size=(seq_length, bs, vocab_size)\n",
    "        return score_seq \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gnTKxy9BbVr"
   },
   "source": [
    "### Build the net. Choose the hidden size to be 128 and the number of heads to be 16. \n",
    "### How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1634566910385,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "zj7UdAZqrP1R",
    "outputId": "f57165f9-f358-486f-8fc3-b0f06555452d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_net(\n",
      "  (layer1): Embedding(10001, 128)\n",
      "  (layer2): ANN(\n",
      "    (encoder): Transformer_encoder(\n",
      "      (TR_Blocks): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock_encoder(\n",
      "          (LN_MHA): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (LN_MLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (MHA): MultipleAttentionHead_encoder(\n",
      "            (MHA): ModuleList(\n",
      "              (0-15): 16 x AttentionHead_encoder(\n",
      "                (query): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (key): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (value): Linear(in_features=128, out_features=8, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (MLP): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Transformer_decoder(\n",
      "      (TR_Blocks): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock_decoder(\n",
      "          (LN_MHA_H): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (LN_MHA_Henc): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (LN_MLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (SA_MHA): SelfAttention_MultipleAttentionHead_decoder(\n",
      "            (MHA): ModuleList(\n",
      "              (0-15): 16 x SelfAttention_AttentionHead_decoder(\n",
      "                (query): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (key): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (value): Linear(in_features=128, out_features=8, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (CA_MHA): CrossAttention_MultipleAttentionHead_decoder(\n",
      "            (MHA): ModuleList(\n",
      "              (0-15): 16 x CrossAttention_AttentionHead_decoder(\n",
      "                (query): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (key): Linear(in_features=128, out_features=8, bias=False)\n",
      "                (value): Linear(in_features=128, out_features=8, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (MLP): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Linear(in_features=128, out_features=10001, bias=True)\n",
      ")\n",
      "There are 3494417 (3.49 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128 \n",
    "num_heads = 16\n",
    "num_blocks = 2\n",
    "seq_length = 100\n",
    "\n",
    "net = attention_net(hidden_size, num_heads, num_blocks, seq_length, dropout=0.0)\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khaDFxmwBbVr"
   },
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1634566910938,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "3XX6XNnArP1S"
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecX1xVaBbVs"
   },
   "source": [
    "### Choose the loss to be the cross-entropy and the optimizer to be Adam, as well as the hyperparameters: \n",
    "* initial learning rate = 0.001\n",
    "* sequence length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1634566911994,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "8mRAllGrrP1S"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 0.001\n",
    "seq_length = 100\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
    "\n",
    "pos = generate_positional_encoding(seq_length, hidden_size).to(device) # size=(seq_length, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frtpNDBLBbVs"
   },
   "source": [
    "### Do 50 passes through the training set\n",
    "### Observe the train perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61136,
     "status": "ok",
     "timestamp": 1634566974730,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "McQc1_xkBbVp",
    "outputId": "71cafe76-e1ff-4951-87b0-7962fd3e2c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 1.5565376281738281 \t lr= 0.001 \t exp(loss)= 3536.0322217216153\n",
      "epoch= 1 \t time= 2.286069631576538 \t lr= 0.001 \t exp(loss)= 677.2619388837608\n",
      "epoch= 2 \t time= 3.010765314102173 \t lr= 0.001 \t exp(loss)= 447.93582434129377\n",
      "epoch= 3 \t time= 3.7287795543670654 \t lr= 0.001 \t exp(loss)= 362.9315671849145\n",
      "epoch= 4 \t time= 4.442428827285767 \t lr= 0.001 \t exp(loss)= 297.601995307535\n",
      "epoch= 5 \t time= 5.162405490875244 \t lr= 0.001 \t exp(loss)= 241.88107698954806\n",
      "epoch= 6 \t time= 5.880281209945679 \t lr= 0.001 \t exp(loss)= 193.3293734169416\n",
      "epoch= 7 \t time= 6.60055947303772 \t lr= 0.001 \t exp(loss)= 152.89467690373093\n",
      "epoch= 8 \t time= 7.313811540603638 \t lr= 0.001 \t exp(loss)= 119.82391985951472\n",
      "epoch= 9 \t time= 8.030363082885742 \t lr= 0.001 \t exp(loss)= 93.32355491859771\n",
      "epoch= 10 \t time= 8.74772310256958 \t lr= 0.001 \t exp(loss)= 72.6485423330933\n",
      "epoch= 11 \t time= 9.46465253829956 \t lr= 0.001 \t exp(loss)= 57.45005196462351\n",
      "epoch= 12 \t time= 10.165895700454712 \t lr= 0.001 \t exp(loss)= 46.46014802831894\n",
      "epoch= 13 \t time= 10.83324384689331 \t lr= 0.001 \t exp(loss)= 36.596026908397235\n",
      "epoch= 14 \t time= 11.502257585525513 \t lr= 0.001 \t exp(loss)= 29.592320975246516\n",
      "epoch= 15 \t time= 12.169542074203491 \t lr= 0.001 \t exp(loss)= 22.862122058308188\n",
      "epoch= 16 \t time= 12.838506937026978 \t lr= 0.001 \t exp(loss)= 17.52679219713118\n",
      "epoch= 17 \t time= 13.510802268981934 \t lr= 0.001 \t exp(loss)= 13.663057642462737\n",
      "epoch= 18 \t time= 14.179578065872192 \t lr= 0.001 \t exp(loss)= 11.538704159140055\n",
      "epoch= 19 \t time= 14.84729552268982 \t lr= 0.001 \t exp(loss)= 10.088870986538119\n",
      "epoch= 20 \t time= 15.516074180603027 \t lr= 0.001 \t exp(loss)= 8.687424425011256\n",
      "epoch= 21 \t time= 16.185720443725586 \t lr= 0.001 \t exp(loss)= 7.253745085685608\n",
      "epoch= 22 \t time= 16.85319471359253 \t lr= 0.001 \t exp(loss)= 6.277685935641339\n",
      "epoch= 23 \t time= 17.52446222305298 \t lr= 0.001 \t exp(loss)= 5.020815757564218\n",
      "epoch= 24 \t time= 18.192822694778442 \t lr= 0.001 \t exp(loss)= 4.170343919136174\n",
      "epoch= 25 \t time= 18.868634462356567 \t lr= 0.001 \t exp(loss)= 3.5331104688160577\n",
      "epoch= 26 \t time= 19.53854775428772 \t lr= 0.001 \t exp(loss)= 3.140647772812609\n",
      "epoch= 27 \t time= 20.205049991607666 \t lr= 0.001 \t exp(loss)= 2.8798741667848575\n",
      "epoch= 28 \t time= 20.87236452102661 \t lr= 0.001 \t exp(loss)= 2.6969763816704933\n",
      "epoch= 29 \t time= 21.54293417930603 \t lr= 0.001 \t exp(loss)= 2.427150567193224\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    doc_len = train_data.size(0)\n",
    "    for count in range( 0 , doc_len-seq_length ,  seq_length): \n",
    "        \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_seq_in = train_data[count:count+seq_length]\n",
    "        start_token = torch.tensor([vocab_size-1]).repeat(1,bs)\n",
    "        minibatch_seq_out = torch.cat((start_token, train_data[count+1:count+seq_length]))\n",
    "        minibatch_label = train_data[count+1:count+seq_length+1]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_seq_in = minibatch_seq_in.to(device)\n",
    "        minibatch_seq_out = minibatch_seq_out.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "        \n",
    "        # forward the minibatch through the net        \n",
    "        scores = net( minibatch_seq_in, minibatch_seq_out, pos ) # size=(seq_length, bs, vocab_size)\n",
    "\n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores = scores.view(  bs*seq_length , vocab_size) # size=(seq_length.bs, vocab_size)\n",
    "        minibatch_label = minibatch_label.view(  bs*seq_length ) # size=(seq_length.bs, vocab_size)\n",
    "       \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    if not epoch%1:\n",
    "        print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRt3XQhgrP1T"
   },
   "source": [
    "### Check if the network was successful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1634567074104,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "49t0myXb6aCy",
    "outputId": "7946e636-dad9-410e-ea89-241124343edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: tensor([  64, 1519,   24,   32, 5166, 3719,   48, 1705, 3940,   32,   26, 1485,\n",
      "          42, 2774, 2196,   32, 5166,   64,   26,  270, 1522, 1978,  108,   32,\n",
      "         833, 2876, 2810, 1342,   32,   26, 2072, 5167,   42, 5164,   26,   98,\n",
      "         108,   35, 5168, 3755,  636,   32, 5159,   48,  729,   64,   65, 2813,\n",
      "        2217,  591,  424,   32, 2476,   83,   93, 1040,  432, 3303,   24,   32,\n",
      "         929,  887,   32,  101, 5166,  546,  315,  895,  108, 1035, 3719,  243,\n",
      "          93,  504,  874,  566, 5169,   64, 5170, 5171,  169,   26, 5166,  156,\n",
      "        5172,   24,   32, 5166,   98,   93,  152, 1881,  135,  623, 2358,   32,\n",
      "          26, 1978,  566,  895], device='cuda:0')\n",
      "\n",
      "Expected output sequence: tensor([1519,   24,   32, 5166, 3719,   48, 1705, 3940,   32,   26, 1485,   42,\n",
      "        2774, 2196,   32, 5166,   64,   26,  270, 1522, 1978,  108,   32,  833,\n",
      "        2876, 2810, 1342,   32,   26, 2072, 5167,   42, 5164,   26,   98,  108,\n",
      "          35, 5168, 3755,  636,   32, 5159,   48,  729,   64,   65, 2813, 2217,\n",
      "         591,  424,   32, 2476,   83,   93, 1040,  432, 3303,   24,   32,  929,\n",
      "         887,   32,  101, 5166,  546,  315,  895,  108, 1035, 3719,  243,   93,\n",
      "         504,  874,  566, 5169,   64, 5170, 5171,  169,   26, 5166,  156, 5172,\n",
      "          24,   32, 5166,   98,   93,  152, 1881,  135,  623, 2358,   32,   26,\n",
      "        1978,  566,  895,   32], device='cuda:0')\n",
      "\n",
      "Predicted output sequence: tensor([  78,   24,   32, 5166, 3719,   48, 1705, 3940,   32,   26, 1485,   42,\n",
      "        2774, 2196,   32, 5166,   64,   26,  108, 1522, 1978,  108,   35,  833,\n",
      "        2876, 2810, 1342,   32,   26, 2072, 5167,   42, 5164,   26,   98,  108,\n",
      "          35, 5168, 3755,  636,   32,  458,  227,  729,   64,   65, 2813, 2217,\n",
      "         591,  424,   32,   26,   83,   93,   40,  432, 3303,  108,   32, 3444,\n",
      "         887,   32, 2315, 5166,  546,   24,  895,  108, 1035, 3719,  243,   93,\n",
      "          32,  874,  566, 5169,   64, 5170, 5171,  169,   26, 2715,   98, 5172,\n",
      "          24,   32,   26,   98,   93,  108, 1881,  135,  623, 2358,   32,   26,\n",
      "          32,  566,  152,   32], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "minibatch_seq_in = train_data[count:count+seq_length, 1].unsqueeze(1).to(device)\n",
    "print('Input sequence:', minibatch_seq_in[:,0])\n",
    "start_token = torch.tensor([vocab_size-1]).repeat(1,bs).to(device)\n",
    "minibatch_seq_out = torch.cat((start_token, train_data[count+1:count+seq_length].to(device)))\n",
    "minibatch_seq_out = minibatch_seq_out[:,1].unsqueeze(1)\n",
    "minibatch_label = train_data[count+1:count+seq_length+1,1].unsqueeze(1).to(device)\n",
    "print('\\nExpected output sequence:', minibatch_label[:,0])\n",
    "pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim) \n",
    "pos = pos.to(device)\n",
    "scores = net( minibatch_seq_in, minibatch_seq_out, pos ) # size=(seq_length, bs, vocab_size)\n",
    "seq = scores.squeeze().argmax(dim=1)\n",
    "print('\\nPredicted output sequence:', seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1634566736663,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "VWXXTLvwVaDc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1634566736664,
     "user": {
      "displayName": "Xavier Bresson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgioGx5OdvAc1VASSVcYQ8NHiQo4PQ7B39ZSmys=s64",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "HxNkId08BbVv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
